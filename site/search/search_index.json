{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Module Descriptor","text":""},{"location":"index.html#general-objectives","title":"General Objectives:","text":"<p>  This module aims to provide a comprehensive understanding of algorithmic foundations, covering fundamental concepts to advanced topics essential for computer science practitioners. Through a structured approach, it aims to familiarise students with the theoretical underpinnings, practical methodologies, and techniques in algorithm design, analysis, and implementation. By delving into mathematical foundations, asymptotic analysis, recurrence relations, divide and conquer strategies, sorting algorithms, dynamic programming, and greedy algorithms, students will gain a holistic view of algorithmic problem-solving.  </p>"},{"location":"index.html#learning-outcomes","title":"Learning outcomes:","text":"<p>On completion of the module, students will be able to:</p> <ol> <li>Define key concepts in number theory, combinatorics, and probability theory as they relate to algorithmic design.</li> <li>Explain the principles of asymptotic analysis and the role of algorithms in computing.</li> <li>Assess the efficiency of different approaches to solving recurrence relations, including the substitution method, recursion-tree method, and master method.</li> <li>Implement divide-and-conquer algorithms such as Strassen's algorithm for matrix multiplication and the Fast Fourier Transform.</li> <li>Compare and contrast various sorting algorithms, evaluating their time complexity and performance characteristics.</li> <li>Design dynamic programming solutions for complex problems such as the rod cutting problem and optimal binary search trees.</li> <li>Develop greedy algorithms for optimization problems, such as the fractional knapsack problem and activity selection.</li> <li>Examine the principles and applications of randomised algorithms in problem-solving. - Critique the effectiveness of different algorithm design paradigms (divide-and-conquer, dynamic programming, greedy) for various problem types.</li> </ol>"},{"location":"index.html#learning-and-teaching-approach","title":"Learning and Teaching Approach","text":"Type Approach Hours per Week Total Credit Hours Contact Lecture/Flipped Classroom 2 60 Practical 2 Independent Study Assignment 2 60 Consultation 1 Self Study 1 Total 120"},{"location":"index.html#assessment-approach","title":"Assessment Approach:","text":"<p>The assessment approach of the following continuous assessments (CA) comprises CA theory - 40%, and CA Practical - 60%, The CA theory consists of a mid-term test, class test, and a quiz. The CA practical will include practical assignments each weighing 15%.</p> <p>A. Mid-term Test: (15%) Students will take a written test of 2-hour duration covering topics up to the middle of the semester.</p> <p>B. Class Test: (15%) Students will take a written test of 1-hour duration covering topics after the mid-term test in the 10th week.</p> <p>C. Quiz: (10%) During the 12th week, students will take an in-class, closed-book quiz. The tutor will organise the quiz, which will cover 70% of the syllabus content.</p> <p>D. Programming Assignments (60%) Students are required to complete four programming assignments throughout the semester. Each assignment will build upon the concepts covered in the lectures and practical sessions.\\ Each assignment shall be assessed out of 15 marks as follows:\\</p> <ul> <li>5 marks: Analysis and problem-solving skills\\</li> <li>5 marks: Application of concepts\\</li> <li>3 marks: Presentation and explanation of solutions\\</li> <li>2 marks: Timely submission and correctness</li> </ul>"},{"location":"index.html#overview-of-the-assessment-approaches-and-weighting","title":"Overview of the assessment approaches and weighting","text":"<p>Areas of Assessment Quantity Weighting (%)</p> <p>Mid Term Test 1 15 Class Test 1 15 Quiz 1 10 Assignments 4 60 Total 100</p> <p>Pre-requisites: CSF101 Programming Methodology</p> <p>Unit 1</p>"},{"location":"answers.html","title":"Answer Keys","text":""},{"location":"answers.html#unit-3","title":"Unit 3","text":""},{"location":"answers.html#2-master-theorem","title":"2. Master Theorem","text":"<ol> <li> <p>Case 3, \\(T(n) = \\Theta (n^2)\\)</p> </li> <li> <p>Case 2, \\(T(n) = \\Theta (n^2 \\log n)\\)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (2^n)\\)</p> </li> <li> <p>Master Theorem does not apply (a is not constant)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n^2)\\)</p> </li> <li> <p>Case 2, \\(T(n) = \\Theta (n \\log^2 n)\\)</p> </li> <li> <p>Does not apply classic MT(non-polynomial difference between f(n) and nlogb a) using other method: \\(T(n) = \\Theta (n \\log \\log n)\\)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (n^{0.51})\\)</p> </li> <li> <p>Does not apply (a &lt; 1)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (n! )\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (\\sqrt n)\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n ^{log 3})\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n)\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n^2)\\)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (n \\log n)\\)</p> </li> <li> <p>Case 2, \\(T(n) = \\Theta (n \\log n)\\)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (n^2 \\log n)\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n^2)\\)</p> </li> <li> <p>Does not apply (f(n) is not positive)</p> </li> <li> <p>Case 3, \\(T(n) = \\Theta (n^2 )\\)</p> </li> <li> <p>Case 1, \\(T(n) = \\Theta (n^2 )\\)</p> </li> </ol>"},{"location":"glossary.html","title":"Glossary","text":"unit2: Algorithms Algorithms are a set of finite rules or instructions to be followed in calculations or other problem-solving operations unit3:Recurrence An equation (or inequality) that defines a function in terms of its value(s) at smaller input(s). It expresses a problem\u2019s solution in terms of smaller subproblems of the same type. Simply, when a function calls itself within that function with different parameters, it refers to smaller subproblems. unit3:Recurrence Relation The actual formula that defines a sequence based on its previous terms. A recurrence relation shows how the current value depends on previous values. Example: Factorial recurrence $ T(n) = n \\cdot T(n-1), T(0) = 1 $ This means the factorial of <code>n</code> is defined in terms of factorial of <code>n-1</code>. unit3:Polynomial Growth Polynomial growth in an algorithm refers to a characteristic where the algorithm's running time or space complexity grows proportionally to a polynomial function of the input size. This means that as the input size, typically denoted by 'n', increases, the resources required by the algorithm (time or memory) increase at a rate that can be expressed as \\(n^{k}\\), where 'k' is a non-negative constant. \\(\\frac{f(n)}{g(n)}\\) where \\(f(n)\\) is the actual running time of the algorithm and \\(g(n)\\) is the reference function, thus their ratio would provide a growth rate of \\(f(n)\\) with respect to \\(g(n)\\). In simple terms, if the for input \"n\" has a a exponential value, we can say that it is a polynomial growth. eg. \\(n^2\\), \\(n^{0.1}\\), or \\(n^{0.7}\\)"},{"location":"questions.html","title":"Questions","text":""},{"location":"questions.html#unit-3-practice-questions","title":"Unit 3: Practice Questions","text":""},{"location":"questions.html#1-iterative-method","title":"1. Iterative method","text":"<p>Here are some common recurrence relations, solve this using iterative method:</p> <ol> <li> <p>$ \\quad     T(n) = T(n+1) + 1, \\quad T(1) = 1 \\quad     $</p> </li> <li> <p>$ \\quad     T(n) = T\\left(\\frac{n}{2} + 1\\right) + 1, \\quad T(1) = 1     $</p> </li> <li> <p>$ \\quad     T(n) = T(n-1) + n, \\quad T(1) = 1     $</p> </li> <li> <p>$ \\quad     T(n) = 2T\\left(\\frac{n}{2}\\right) + n, \\quad T(1) = 1     $</p> </li> <li> <p>$ \\quad     T(n) = T(n-1) + 2, \\quad T(1) = 1     $</p> </li> <li> <p>$ \\quad     T(n) = T\\left(\\frac{n}{2}\\right) + n, \\quad T(1) = 1     $</p> </li> <li> <p>$ \\quad     T(n) = 2T\\left(\\frac{n}{2}\\right) + n^2, \\quad T(1) = 1     $</p> </li> </ol>"},{"location":"questions.html#2-master-theorem","title":"2. Master Theorem","text":"<p>For each of the following recurrences, determine the asymptotic runtime \ud835\udc47(\ud835\udc5b).</p> <ol> <li> <p>If the recurrence can be solved using the classic Master Theorem, apply it directly and provide the solution.</p> </li> <li> <p>If the classic Master Theorem does not apply, state this explicitly and then attempt to solve the recurrence using the generalized or extended Master Theorem, where applicable.</p> </li> </ol> <p>1)  \\(T(n) = 3T\\left(\\tfrac{n}{2}\\right) + n^2\\)</p> <p>2)  \\(T(n) = 4T\\left(\\tfrac{n}{2}\\right) + n^2\\)</p> <p>3)  \\(T(n) = T\\left(\\tfrac{n}{2}\\right) + 2^n\\)</p> <p>4)  \\(T(n) = 2^n \\, T\\left(\\tfrac{n}{2}\\right) + n ^ n\\)</p> <p>5)  \\(T(n) = 16T\\left(\\tfrac{n}{4}\\right) + n\\)</p> <p>6)  \\(T(n) = 2T\\left(\\tfrac{n}{2}\\right) + n \\log n\\)</p> <p>7)  \\(T(n) = 2T\\left(\\tfrac{n}{2}\\right) + \\tfrac{n}{\\log n}\\)</p> <p>8)  \\(T(n) = 2T\\left(\\tfrac{n}{4}\\right) + n^{0.51}\\)</p> <p>9)  \\(T(n) = 0.5T\\left(\\tfrac{n}{2}\\right) + \\tfrac{1}{n}\\)</p> <p>10) \\(T(n) = 16T\\left(\\tfrac{n}{4}\\right) + n!\\)</p> <p>11) \\(T(n) = \\sqrt{2} \\, T\\left(\\tfrac{n}{2}\\right) + \\log n\\)</p> <p>12) \\(T(n) = 3T\\left(\\tfrac{n}{2}\\right) + n\\)</p> <p>13) \\(T(n) = 3T\\left(\\tfrac{n}{3}\\right) + \\sqrt{n}\\)</p> <p>14) \\(T(n) = 4T\\left(\\tfrac{n}{2}\\right) + cn\\)</p> <p>15) \\(T(n) = 3T\\left(\\tfrac{n}{4}\\right) + n \\log n\\)</p> <p>16) \\(T(n) = 3T\\left(\\tfrac{n}{3}\\right) + \\tfrac{n}{2}\\)</p> <p>17) \\(T(n) = 6T\\left(\\tfrac{n}{3}\\right) + n^2 \\log n\\)</p> <p>18) \\(T(n) = 4T\\left(\\tfrac{n}{2}\\right) + \\tfrac{n}{\\log n}\\)</p> <p>19) \\(T(n) = 64T\\left(\\tfrac{n}{8}\\right) - n^2 \\log n\\)</p> <p>20) \\(T(n) = 7T\\left(\\tfrac{n}{3}\\right) + n^2\\)</p> <p>21) \\(T(n) = 4T\\left(\\tfrac{n}{2}\\right) + \\log n\\)</p>"},{"location":"questions.html#unit-4-practice-questions","title":"Unit 4: Practice Questions","text":""},{"location":"questions.html#answers","title":"Answers","text":"<p>Answers</p>"},{"location":"unit2.html","title":"Unit 2","text":"<p>Learning Outcome</p> <p>Explain the principles of asymptotic analysis and the role of algorithms in computing.</p>"},{"location":"unit2.html#21-fundamentals","title":"2.1 Fundamentals","text":"<p> are a set of finite rules or instructions to be followed in calculations or other problem-solving operations"},{"location":"unit2.html#211-features-of-algorithm","title":"2.1.1 Features of Algorithm","text":"<p>Every algorithm must satisfy the following Criteria:</p> <ol> <li> <p>Input: there are zero or more quantities, which are externally supplied;</p> </li> <li> <p>Output: at least one quantity is produced;</p> </li> <li> <p>Definiteness: each instruction must be clear and unambiguous;</p> </li> <li> <p>Finiteness if we trace out the instructions of an algorithm, then for all cases the algorithm will terminate after a finite number of steps;</p> </li> <li> <p>Effectiveness: every instruction must be sufficiently basic that it can in    principle be carried out by a person using only pencil and paper. It is not enough that each operation be definite, but it must also be feasible.</p> </li> </ol>"},{"location":"unit2.html#22-asymptotic-analysis","title":"2.2 Asymptotic Analysis","text":"<ul> <li>Asymptotic Analysis : Asymptotic analysis of an algorithm studies its efficiency in terms of input size n as n\u2192\u221e. It provides an estimate of time or space complexity using notations bigO. Theta , omega and is independent of hardware or implementation.</li> </ul>"},{"location":"unit2.html#211-concept-of-growth-rates","title":"2.1.1 Concept of Growth Rates","text":"<ul> <li> <p>Growth rate describes how the running time of an algorithm increases as the input size grows. We consider only the leading term of a formula (e.g., an<sup>2</sup>), since the lower-order terms are   relatively insignificant for large values of n.</p> </li> <li> <p>We also ignore the leading term\u2019s constant coefficient, since constant factors are less significant than the rate of growth in determining computational efficiency for large inputs.</p> </li> <li> <p>Example: Linear (O(n)), Quadratic (O(n\u00b2)), Logarithmic (O(log n)).</p> </li> <li>Helps compare algorithms independently of hardware.</li> </ul> <p></p> <p>For lower value of input some worst case may perform better</p> <p>We usually consider one algorithm to be more efficient than another if its worstcase running time has a lower order of growth. Due to constant factors and lowerorder terms, an algorithm whose running time has a higher order of growth might take less time for small inputs than an algorithm whose running time has a lower2.3 Designing algorithms 29 order of growth. But for large enough inputs, a \u201a.n2/ algorithm, for example, will run more quickly in the worst case than a \u201a.n3/ algorithm.</p>"},{"location":"unit2.html#212-best-case-worst-case-and-average-case-analysis","title":"2.1.2 Best-case, Worst-case, and Average-case Analysis","text":"<ul> <li>Best-case: Minimum time an algorithm takes for any input (optimistic).</li> <li>Worst-case: Maximum time an algorithm takes (pessimistic, most commonly used).</li> <li>Average-case: Expected time over all inputs (realistic measure).</li> </ul>"},{"location":"unit2.html#213-importance-of-asymptotic-analysis-in-algorithm-design","title":"2.1.3 Importance of Asymptotic Analysis in Algorithm Design","text":"<ul> <li>Provides machine-independent evaluation of algorithms.</li> <li>Helps predict scalability for large input sizes.</li> <li>Guides selection of efficient algorithms.</li> </ul>"},{"location":"unit2draft.html","title":"Unit2draft","text":""},{"location":"unit2draft.html#22-model-of-computation-word-ram","title":"2.2 Model of Computation (Word-RAM)","text":""},{"location":"unit2draft.html#221-basic-operations-and-their-time-complexity","title":"2.2.1 Basic Operations and Their Time Complexity","text":"<ul> <li>Arithmetic (+, -, *, /), comparisons, and assignments assumed O(1).</li> <li>Array access and pointer dereferencing also O(1).</li> </ul>"},{"location":"unit2draft.html#222-memory-model-and-addressing","title":"2.2.2 Memory Model and Addressing","text":"<ul> <li>Memory is modeled as an array of words (fixed size).</li> <li>Each word can be accessed in constant time.</li> <li>Suitable for analyzing most modern computers.</li> </ul>"},{"location":"unit2draft.html#223-comparison-with-other-computation-models","title":"2.2.3 Comparison with Other Computation Models","text":"<ul> <li>Turing Machine: Theoretical, less practical for real hardware analysis.</li> <li>PRAM (Parallel RAM): Models parallel computing.</li> <li>Word-RAM: Balances practicality and simplicity.</li> </ul>"},{"location":"unit2draft.html#23-role-of-algorithms-in-computing","title":"2.3 Role of Algorithms in Computing","text":""},{"location":"unit2draft.html#231-historical-perspective-on-algorithms","title":"2.3.1 Historical Perspective on Algorithms","text":"<ul> <li>Algorithms date back to ancient mathematics (Euclid\u2019s GCD algorithm).</li> <li>Modern computing relies on algorithmic foundations.</li> </ul>"},{"location":"unit2draft.html#232-importance-of-efficient-algorithms-in-modern-computing","title":"2.3.2 Importance of Efficient Algorithms in Modern Computing","text":"<ul> <li>Efficiency saves time and resources.</li> <li>Critical in big data, AI, cybersecurity, and distributed systems.</li> </ul>"},{"location":"unit2draft.html#233-real-world-applications-of-algorithm-analysis","title":"2.3.3 Real-world Applications of Algorithm Analysis","text":"<ul> <li>Search engines, data compression, cryptography, scheduling, and machine learning.</li> </ul>"},{"location":"unit2draft.html#24-analyzing-and-designing-algorithms","title":"2.4 Analyzing and Designing Algorithms","text":""},{"location":"unit2draft.html#241-principles-of-algorithm-design","title":"2.4.1 Principles of Algorithm Design","text":"<ul> <li>Divide and Conquer, Dynamic Programming, Greedy strategies.</li> <li>Aim for correctness, clarity, and efficiency.</li> </ul>"},{"location":"unit2draft.html#242-trade-offs-between-time-and-space-complexity","title":"2.4.2 Trade-offs Between Time and Space Complexity","text":"<ul> <li>Faster algorithms may use more memory (e.g., precomputed tables).</li> <li>Memory-efficient algorithms may be slower.</li> </ul>"},{"location":"unit2draft.html#25-o-notation-notation-and-notation","title":"2.5 O-notation, \u03a9-notation, and \u0398-notation","text":""},{"location":"unit2draft.html#251-o-notation-upper-bound","title":"2.5.1 O-notation (Upper Bound)","text":"<ul> <li>Describes the worst-case upper limit on growth rate.</li> <li>Example: f(n) = 3n\u00b2 + 2n + 1 \u2208 O(n\u00b2).</li> </ul>"},{"location":"unit2draft.html#252-notation-lower-bound","title":"2.5.2 \u03a9-notation (Lower Bound)","text":"<ul> <li>Describes the minimum growth rate of an algorithm.</li> <li>Example: f(n) = 3n\u00b2 + 2n + 1 \u2208 \u03a9(n\u00b2).</li> </ul>"},{"location":"unit2draft.html#253-notation-tight-bound","title":"2.5.3 \u0398-notation (Tight Bound)","text":"<ul> <li>When both upper and lower bounds are the same.</li> <li>Example: f(n) = 3n\u00b2 + 2n + 1 \u2208 \u0398(n\u00b2).</li> </ul>"},{"location":"unit2draft.html#254-relationships-between-notations","title":"2.5.4 Relationships Between Notations","text":"<ul> <li>If f(n) \u2208 \u0398(g(n)), then f(n) \u2208 O(g(n)) and f(n) \u2208 \u03a9(g(n)).</li> </ul>"},{"location":"unit2draft.html#26-standard-notation-common-functions","title":"2.6 Standard Notation &amp; Common Functions","text":""},{"location":"unit2draft.html#261-polynomial-functions-and-their-growth-rates","title":"2.6.1 Polynomial Functions and Their Growth Rates","text":"<ul> <li>n, n\u00b2, n\u00b3 \u2026 grow faster as degree increases.</li> <li>Example: n\u00b3 grows faster than n\u00b2.</li> </ul>"},{"location":"unit2draft.html#262-logarithmic-and-exponential-functions","title":"2.6.2 Logarithmic and Exponential Functions","text":"<ul> <li>log n grows very slowly, exponential (2\u207f) grows very fast.</li> <li>Example: log\u2082(1024) = 10, but 2\u00b9\u2070 = 1024.</li> </ul>"},{"location":"unit2draft.html#263-floor-and-ceiling-functions","title":"2.6.3 Floor and Ceiling Functions","text":"<ul> <li>Floor \u230ax\u230b: Largest integer \u2264 x.</li> <li>Ceiling \u2308x\u2309: Smallest integer \u2265 x.</li> </ul>"},{"location":"unit2draft.html#264-factorial-and-combinatorial-functions","title":"2.6.4 Factorial and Combinatorial Functions","text":"<ul> <li>n! = n \u00d7 (n-1) \u00d7 \u2026 \u00d7 1 (grows extremely fast).</li> <li>Useful in probability, combinatorics, and complexity analysis.</li> </ul>"},{"location":"unit3.html","title":"Unit 3: Recurrence Relations and Master Theorem","text":"<p>Learning outome</p> <p>Assess the efficiency of different approaches to solving recurrence relations, including the substitution method, recursion-tree method, and master method.</p>"},{"location":"unit3.html#31-recurrences","title":"3.1 Recurrences","text":""},{"location":"unit3.html#311-introduction","title":"3.1.1 Introduction","text":"<p>What is ? <p>A recurrence is an equation (or inequality) that defines a function in terms of its value(s) at smaller input(s).It\u2019s a way of expressing a problem\u2019s solution in terms of smaller subproblems of the same type. In simple terms, we can say that when a function calls itself within that function, only the parameter are different, which is usally the smaller sub problem.</p> <p>What is a recurrence relations?</p> <p>A recurrence relation is an  equation that defines a sequence based on its previous terms . A recurrence relation is the actual formula that shows how the current value depends on previous values.</p> <p>For example:</p> <p>Factorial recurrence: $ T(\ud835\udc5b)=\ud835\udc5b.T(\ud835\udc5b-1), T(0)=1 $</p> <p>This means factorial of \ud835\udc5b is defined in terms of factorial of \ud835\udc5b\u22121.</p>"},{"location":"unit3.html#312-general-form","title":"3.1.2 General Form","text":"\\[ T(n) = a \\cdot T\\!\\left(\\frac{n}{b}\\right) + f(n) \\] <p>Where:</p> <ul> <li>\\(T(n)\\) \u2192 the time/space complexity of the problem of size \\(n\\)</li> <li>\\(a\\) \u2192 number of subproblems in the recursion</li> <li>\\(n/b\\) \u2192 size of each subproblem</li> <li>\\(f(n)\\) \u2192 the cost of dividing the problem and combining the results</li> </ul>"},{"location":"unit3.html#313-types-of-recurrence-relations","title":"3.1.3 Types of Recurrence Relations","text":"<ol> <li>Linear Recurrence \u2013 depends linearly on smaller subproblems.     Example: \\(T(n) = T(n-1) + O(1)\\)</li> <li> <p>Divide-and-Conquer Recurrence \u2013 splits into multiple subproblems.     Example: \\(T(n) = aT(\\frac{n}{b}) + f(n)\\)</p> <ul> <li>Homogeneous: depends only on subproblems (\\(T(n) = 2T(\\frac{n}{2})\\)).</li> <li>Non-Homogeneous: includes an additional function (\\(T(n) = 2T(\\frac{n}{2}) + n\\)).</li> </ul> </li> </ol>"},{"location":"unit3.html#32-methods-of-solving-recurrence-relations","title":"3.2 Methods of Solving Recurrence Relations","text":"<p>When analyzing recursive algorithms, we use different techniques to solve recurrence relations and find their closed-form or asymptotic complexity. Here are the main methods:</p> <ol> <li>Iterative Method</li> <li>Recursion tree</li> <li>Substitution Method</li> <li>Telescoping</li> <li>Master theorem</li> </ol>"},{"location":"unit3.html#321-iterative-method-expansion-method","title":"3.2.1. Iterative Method (Expansion Method)","text":"<ul> <li>Expand the recurrence step by step until a clear pattern emerges.</li> <li>Replace the recurrence with successive substitutions.</li> <li>Stop once the base case is reached, then simplify.</li> </ul> <p>Example 1:</p> <p>$$ T(n) = T(n-1) + 1, \\quad T(1) = 1 $$</p> <p>We\u2019ll solve it by unrolling (expanding) step by step until the base case.</p> <p>Step 1. Unroll the Recurrence</p> <p>Start expanding:</p> <p>$$ T(n) = T(n-1) + 1 $$</p> <p>$$ T(n-1) = T(n-1-1) + 1  \\quad\\implies\\quad T(n) = T(n-2) + 2 $$</p> <p>$$ T(n-2) = T(n-2-1) + 1 + 1 \\quad\\implies\\quad T(n) = T(n-3) + 3 $$</p> <p>$$ T(n-3) = T(n-3-1) + 1 + 1 + 1 + 1 \\quad\\implies\\quad T(n) = T(n-4) + 4 $$</p> <p>After \\(k\\) steps, the pattern is:</p> <p>$$      T(n) = T(n-k) + k $$</p> <p>Step 2. Stop at the Base Case</p> <p>Pick \\(k\\) so that \\(n - k\\) hits the base index \\(b\\) (where \\(b=1\\) in this case):</p> <p>$$ n - k = 1 \\quad\\Rightarrow\\quad k = n - 1 $$</p> <p>Now substitute \\(k = n - 1\\) into the pattern:</p> <p>$$   T(n) = T(1) + (n - 1)   $$</p> <p>Step 3. Plug In Your Base</p> <p>$$   T(n) = T(1) + (n-1) \\newline         = 1 + (n-1)   $$</p> <p>$ \\therefore $ the growth is linear.</p> <p>step 4. Final Result (Asymptotics)</p> <p>$$   \\boxed{T(n) = \\Theta(n)}   $$</p> <p>Intuition: each step reduces \\(n\\) by 1 and adds a constant cost \\(+1\\), so the total work scales linearly with \\(n\\).</p>"},{"location":"unit3.html#try-this-question","title":"Try this Question","text":"Q1. $ \\quad T(n) = T(n-1)+n, \\quad T(1) = 1 $  <p>$$ T(n) = T(n-1)+n, \\quad T(1) = 1 $$</p> <p>Step 1. Unroll the Recurrence</p> <p>Start expanding:</p> <p>$$ T(n) = T(n-1) + n $$</p> <p>$$ T(n-1) = T(n-1-1) + n-1  \\quad\\implies\\quad T(n) = (T(n-2) + n-1) + n $$</p> <p>$$ T(n-2) = T(n-2-1) + n-2 \\quad\\implies\\quad T(n) = ((T(n-3)+ n-2) + n-1) + n $$</p> <p>$$ T(n-3) = T(n-3-1) + n-3  \\quad\\implies\\quad T(n) = ((T(n-4)+n-3)+n-2)+n-1+n $$</p> <p>After \\(k\\) steps, the pattern is:</p> <p>$$      T(n) = T(n-k) + (n-k+1) + (n-k+2) + (n-k+3) ... n $$</p> <p>Step 2. Stop at the Base Case</p> <p>Pick \\(k\\) so that \\(n - k\\) hits the base index 1:</p> <p>$$ n - k = 1 \\quad\\Rightarrow\\quad k = n - 1 $$</p> <p>Now substitute \\(k = n - 1\\) into the pattern:</p> <p>$$   T(n) = T(1) + 2 +3 + 4 +...+n   $$</p> <p>Step 3. Plug In Your Base</p> <p>$$   T(n) = T(1) + 2 + 3 + 4 + ... + (n-1) + n \\newline         = 1 + 2 + 3 + 4 + ... + n   $$</p> <p>Since we can observe that the algorithm tends to behave as a sum of first \\(n\\) integers</p> <p>$$ 1 + 2 + 3 + 4 + ... + n = \\frac{n(n+1)}{2} $$</p> <p>So,  $$ T(n) = \\frac{n(n+1)}{2} $$</p> <p>step 4. Final Result (Asymptotics)</p> <p>$$   \\boxed{T(n) = \\Theta(n^2)}   $$</p>  Q2. $ \\quad T(n) = T(n/2)+1, \\quad T(1) = 1 $  <p>$$ T(n) = T(n/2)+1, \\quad T(1) = 1 $$</p> <p>Step 1. Unroll the Recurrence</p> <p>Start expanding:</p> <p>$$ T(n) = T(n/2) + 1 $$</p> <p>$$ T(n/2) = T(n/4) + 1  \\quad\\implies\\quad T(n) = T(n/4) +  2 $$</p> <p>$$ T(n/4) = T(n/8) + 2 \\quad\\implies\\quad T(n) = T(n/8)+ 3 $$</p> <p>$$ T(n/8) = T(n/16) + 3  \\quad\\implies\\quad T(n) = T(n/16) + 4 $$</p> <p>After \\(k\\) steps, the pattern is:</p> <p>$$      T(n) = T(n/2^k) + k $$</p> <p>Step 2. Stop at the Base Case</p> <p>Pick \\(k\\) so that \\(n/2^k\\) hits the base index 1:</p> <p>$$ n/2^k = 1 \\quad\\Rightarrow\\quad k = c . log n $$</p> <p>Now substitute \\(k = clog n\\) into the pattern:</p> <p>$$   T(n) = T(1) + c.log n   $$</p> <p>Step 3. Plug In Your Base</p> <p>$$   T(n) = T(1) + c.log n \\newline         = 1 + c.logn   $$</p> <p>step 4. Final Result (Asymptotics)</p> <p>$$   \\boxed{T(n) = \\Theta (logn)}   $$</p>"},{"location":"unit3.html#practice-set-1-of-practice-questions","title":"Practice set 1 of Practice Questions:","text":""},{"location":"unit3.html#322-recursion-tree-method","title":"3.2.2 Recursion Tree Method","text":"<p>In a recursion tree, each node represents the cost of a single subproblem somewhere in the set of recursive function invocations. We sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion</p> <p>Steps</p> <ol> <li> <p>Write the recurrence $ \\quad T(n)=aT(n/b)+f(n) $</p> </li> <li> <p>Expand the recurrence (first few levels)</p> <pre><code>        1. Root: cost = \ud835\udc53(\ud835\udc5b)\n        2. Level 1: \ud835\udc4e subproblems, each of size (\ud835\udc5b/\ud835\udc4f)\n        3. cost per node = \ud835\udc53(\ud835\udc5b/\ud835\udc4f)\n        4. Total cost at level 1 = \ud835\udc4e\u22c5\ud835\udc53(\ud835\udc5b/\ud835\udc4f)\n</code></pre> </li> <li> <p>Generalize the cost at level \ud835\udc56</p> \\[ Level\u00a0cost = \ud835\udc4e^\ud835\udc56\u22c5\ud835\udc53(\ud835\udc5b/\ud835\udc4f^\ud835\udc56) \\] </li> <li> <p>Find the depth of the tree</p> <pre><code>    Stop when subproblem size = 1\n</code></pre> </li> <li> <p>Add up costs across levels</p> </li> </ol> \\[ T(n) = \\sum_{i=0} ^ {\\log_b n} a^i \\cdot f\\!\\left(\\frac{n}{b^i}\\right) \\] <p>Evaluate the sum Simplify the expression to get asymptotic complexity (\u0398, O, etc.).</p> <p>Example 1:</p> <ul> <li>for a given recurrence relation $\\quad T(n)=2T(n/2)+n $</li> <li>Level 0 : root node</li> </ul> <p><pre><code>      flowchart TD\n      A@{shape : rect, label: \"n\"}\n\n</code></pre> </p> <ul> <li>Level 1</li> </ul> <p>Cost of Level 1</p> <p>Level\u00a0cost = \ud835\udc4e^\ud835\udc56\u22c5 \ud835\udc53(\ud835\udc5b/\ud835\udc4f^\ud835\udc56)</p> <p>cost =$ \\frac{n}{2}+\\frac{n}{2} = 2\\frac{n}{2}= cn $</p> <p>At Level 1</p> <p>The problem will be subdivided into 2 numbers with a cost of (n/2)</p> <p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n</code></pre> </p> <ul> <li>Level 2</li> </ul> <p>Cost of Level 2</p> <p>Level\u00a0cost = \ud835\udc4e^\ud835\udc56\u22c5 \ud835\udc53(\ud835\udc5b/\ud835\udc4f^\ud835\udc56)</p> <p>cost = $ \\frac{n}{4}+\\frac{n}{4}+\\frac{n}{4}+\\frac{n}{4} = 4\\frac{n}{4} = cn $</p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n\n      B1 --&gt; C1[\"n/4\"]\n      B1 --&gt; C2[\"n/4\"]\n\n      B2 --&gt; C3[\"n/4\"]\n      B2 --&gt; C4[\"n/4\"]\n</code></pre> <ol> <li>Until i level</li> </ol> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n\n      B1 --&gt; C1[\"n/4\"]\n      B1 --&gt; C2[\"n/4\"]\n\n      B2 --&gt; C3[\"n/4\"]\n      B2 --&gt; C4[\"n/4\"]\n\n      C1 --&gt; D1[\"T(1)\"]\n      C2 --&gt; D2[\"T(1)\"]\n      C3 --&gt; D3[\"T(1)\"]\n      C4 --&gt; D4[\"T(1)\"]\n</code></pre> <ol> <li>Total Cost   From the tree we observe that the sum at each level of \"cn\", thus we can conclude for depth till \"i\" , the sum will be i * n or as per the formula</li> </ol> <p>$$   T(n) = \\sum_{i=0} ^ {\\log_2 n} cn \\implies T(n) = cn \\sum_{i=0} ^ {\\log_2 n} 1   $$</p> <p>$$   T(n) = cn . {\\log n} \\implies \\Theta (nlogn)   $$</p> <p>Final Result (Asymptotics)</p> <p>$$   \\boxed{T(n) = \\Theta (nlogn)}   $$</p>"},{"location":"unit3.html#try-this-question_1","title":"Try this Question","text":"Q1. $ \\quad T(n) = 3T(n/2)+n, \\quad T(1) = 1 $  <ul> <li>for a given recurrence relation $\\quad T(n)=3T(n/2)+ cn $</li> <li>Level 0 : root node</li> </ul> <p><pre><code>      flowchart TD\n      A@{shape : rect, label: \"n\"}\n\n</code></pre> </p> <ul> <li>Level 1</li> </ul> <p>Cost of Level 1</p> <p>Level\u00a0cost = \ud835\udc4e^\ud835\udc56\u22c5 \ud835\udc53(\ud835\udc5b/\ud835\udc4f^\ud835\udc56)</p> <p>cost = $ \\frac{n}{2} + \\frac{n}{2} + \\frac{n}{2} = 3\\frac{n}{2}  $</p> <p>At Level 1</p> <p>subdivided into 3 numbers with a cost of (n/2)</p> <p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n      A --&gt; B3[\"n/2\"]\n</code></pre> </p> <ul> <li>Level 2</li> </ul> <p>Cost of Level 2</p> <p>cost = $ 3\\frac{n}{4} + 3\\frac{n}{4} + 3\\frac{n}{4} \\newline = 9\\frac{n}{4} \\newline = (\\frac {3}{2})^2 n $</p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n      A --&gt; B3[\"n/2\"]\n\n      B1 --&gt; C1[\"n/4\"]\n      B1 --&gt; C2[\"n/4\"]\n      B1 --&gt; C3[\"n/4\"]\n\n      B2 --&gt; C4[\"n/4\"]\n      B2 --&gt; C5[\"n/4\"]\n      B2 --&gt; C6[\"n/4\"]\n\n      B3 --&gt; C7[\"n/4\"]\n      B3 --&gt; C8[\"n/4\"]\n      B3 --&gt; C9[\"n/4\"]\n</code></pre> <ol> <li>Level 3</li> </ol> <p>Cost of Level 3</p> <p>cost = $ 3\\frac{n}{8} + 3\\frac{n}{8} + 3\\frac{n}{8} ... \\newline = 36\\frac{n}{8} \\newline = (\\frac {3}{2})^3 n $</p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n      A --&gt; B3[\"n/2\"]\n\n      B1 --&gt; C1[\"n/4\"]\n      B1 --&gt; C2[\"n/4\"]\n      B1 --&gt; C3[\"n/4\"]\n\n      B2 --&gt; C4[\"n/4\"]\n      B2 --&gt; C5[\"n/4\"]\n      B2 --&gt; C6[\"n/4\"]\n\n      B3 --&gt; C7[\"n/4\"]\n      B3 --&gt; C8[\"n/4\"]\n      B3 --&gt; C9[\"n/4\"]\n\n      C1 --&gt; D1[\"n/8\"]\n      C1 --&gt; D2[\"n/8\"]\n      C1 --&gt; D3[\"n/8\"]\n\n\n</code></pre> <p>Until i level</p> <p>Cost of Level i</p> <p>cost =  $ (\\frac {3}{2})^i n $</p> <pre><code>      flowchart TD\n      A[\"n\"] --&gt; B1[\"n/2\"]\n      A --&gt; B2[\"n/2\"]\n      A --&gt; B3[\"n/2\"]\n\n      B1 --&gt; C1[\"n/4\"]\n      B1 --&gt; C2[\"n/4\"]\n      B1 --&gt; C3[\"n/4\"]\n\n      B2 --&gt; C4[\"n/4\"]\n      B2 --&gt; C5[\"n/4\"]\n      B2 --&gt; C6[\"n/4\"]\n\n      B3 --&gt; C7[\"n/4\"]\n      B3 --&gt; C8[\"n/4\"]\n      B3 --&gt; C9[\"n/4\"]\n\n      C1 --&gt; D1[\"n/8\"]\n      C1 --&gt; D2[\"n/8\"]\n      C1 --&gt; D3[\"n/8\"]\n\n\n      D1 --&gt; E1[\"T(1)\"]\n      D2 --&gt; E2[\"T(1)\"]\n      D3 --&gt; E3[\"T(1)\"]\n\n</code></pre> <p>Total Cost = $ \\sum $ cost of each level</p> <p>$ = n + \\frac{3}{2} + \\frac{9}{4} + \\frac{27}{8} + ... + (\\frac{3}{2})^i $   $ \\newline = n (1 + \\frac{3}{2} + (\\frac{3}{2})^2 + (\\frac{3}{2})^3 + .... + (3/2)^i) $</p> <p>We can observe that the sum of all the levels tends to follow the summation of n numbers for a GEOMETRIC SERIES pattern, thus we can reduce it in the form of sum of GP series</p> <p>$ \\newline = n (\\frac{ 1 -(3/2)^{k+1}}{1- 3/2})  $</p> <p>Sum of Geometric Series</p> <p>$$ S= a( \\frac{1-r^n}{1-r}) $$  where  r = common ratio, n = number of terms, a = first term</p> <p>on solving:</p> <p>$$ T(n) = 2n((3/2)^{k+1}-1) $$</p> <p>as k tends to $ \\infty $ , -1 becomes negligible</p> <p>$$ T(n) \\approx 2n((3/2)^{k+1}) $$</p> <p>and the value for k = height of tree, k = log n</p> <p>$$   \\approx 2n((3/2)^{\\log_2 n +1})   $$   $$   \\approx 2n((3/2)^{\\log_2 n})(3/2)^{1}   $$   $$   \\approx 3n((3/2)^{\\log_2 n})   $$</p> <p>Log Property</p> <p>$$ a^{ \\log_b n} = n^{\\log_b a} $$</p> <p>$$   \\approx 3n((n)^{\\log_2 3/2})   $$   $$   \\approx 3((n)^{1 + \\log_2 3/2})   $$</p> <p>simplfy the exponent part   $$   1 + \\log_2 3/2 \\newline   = 1+ \\log_2 3 - \\log_2 2   = \\log_2 3   $$</p> <p>therefore   $$   T(n) \\approx 3n^{log_2 3}   $$</p> <p>Final Result (Asymptotics)</p> <p>$$   \\boxed{T(n) = \\Theta (n^{1.585})}   $$</p>"},{"location":"unit3.html#323-substitution-method","title":"3.2.3 Substitution Method","text":"<p>It is a method used to solve a recurrence relation by proving the guessed form of the relation.</p> <p>The substitution method for solving recurrences comprises two steps:</p> <ol> <li>Guess the form of the solution.</li> <li>Use mathematical induction to find the constants and show that the solutionworks.    We substitute the guessed solution for the function when applying the inductive hypothesis to smaller values; hence the name \u201csubstitution method.\u201d</li> </ol> <p>Example 1</p> <p>Note: General guess</p> <ul> <li>For divide-and-conquer recurrences = \ud835\udc47(\ud835\udc5b)=\ud835\udc42(\ud835\udc5blog\u2061\ud835\udc5b).</li> <li>For simple ones like \ud835\udc47(\ud835\udc5b)=\ud835\udc47(\ud835\udc5b\u22121)+\ud835\udc5b,  guess a polynomial.</li> </ul> <ul> <li> <p>step 1: Given recurrence relation          $\\quad T(n)=2T(\\frac{n}{2})+n, \\quad T(1) =1 $</p> </li> <li> <p>step 2: Making a informed guess ( say using iterative or recursion tree)         $$ T(n) = \\Theta (n\\log n) $$</p> </li> </ul> <p>Testing the basis:</p> <p>We\u2019ll try to prove that the guess for upper bound T(n)=O(nlog\u2061n) is true for all cases using induction method</p> <p>By definition : $ T(n) \u2264 c(nlogn) \\quad for \\quad \u018e c &gt; 0, \u2200n &gt; n0 $</p> <p>Basis : for $ n = 1, \\newline T(1) \u2264 c(1log1) : fails, \\therefore n &gt; 1 $</p> <p>Basis : for $ n = 2 \\newline   T(2) \u2264 c(2log2) \\newline   4 \u2264 2c  $</p> <p>Therefore we need value of c &gt;2 and n&gt;1   So we can conclude that the eq 1 hold true for all value of n \u2265 2,   i.e 2, 3, 4 ,.... Kth term</p> <ul> <li>step 3: Inductive Hypothesis</li> </ul> <p>Assuming n = k for all value of $ 2 \u2264 k \u2264 n $ $$ T(k) \u2264 c(klogk)  $$</p> <p>It implies that the condition would hold true for $ k = n/2 $     $$ T(\\frac{n}{2}) \u2264 c(\\frac{n}{2} \\log \\frac{n}{2}) $$</p> <ul> <li>step 4: Substitute it back   $$ Thus, \\quad T(n) = 2T(\\frac{n}{2}) + n \\newline $$     $$\u2264 2 (c(\\frac{n}{2} \\log \\frac{n}{2}))  + n          $$     $$ \u2264 cn (\\log \\frac{n}{2})  + n         $$   $$ \u2264 cn (\\log n - 1) + n $$   $$ \u2264 cn log n + n (1 - C) $$   $$ \\approx T(n) = \\Theta( n \\log n ) $$   Hence proved</li> </ul>"},{"location":"unit3.html#try-this-question_2","title":"Try this Question","text":"Q1. $ \\quad T(n) = 2T(n-1)+1, \\quad T(1) = 1 $  <ul> <li> <p>step 1: Given recurrence relation   $$ \\quad T(n) = 2T(n-1)+1, \\quad T(1) = 1 $$</p> </li> <li> <p>step 2 : Guessing a solution   $$ T(n)=2^n - 1 $$</p> <p>for upper bound $ T(n) \u2264 \\Theta (2^n - 1) $</p> <p>Basis for n = 1   $$ T(1) \u2264 C(1), Holds True$$</p> <p>So we can conclude that the eq 1 hold true for all value of n \u2265 1,   i.e 1, 2, 3, 4 ,.... Kth term</p> </li> <li> <p>step 3 : Inductive Hypothesis</p> <p>Assuming n  \u2265 k for all values 1 \u2264 k \u2264 n             $$ T(k) \u2264 c(2^k - 1) $$</p> <p>Therefore $ K= n-1 $ should also hold true             $$ T(n-1) \u2264 c(2^{n-1} - 1) $$</p> <p>Thus, $$T(n) = 2T(n-1) + 1 $$           $$ \u2264 2 (c(2^{n-1} - 1)) + 1 $$           $$ \u2264 2c2^{n-1} - 2  + 1 $$           $$ \u2264 c2^n - 1 $$           Hence proved</p> </li> </ul>  Q1. $ \\quad T(n) = T(n-1)+n, \\quad T(1) = 1 $  <ul> <li> <p>step 1: Given recurrence relation   $$ \\quad T(n) = T(n-1)+n, \\quad T(1) = 1 $$</p> </li> <li> <p>step 2 : Guessing a solution   $$ T(n)=\\Theta (n^2) $$</p> <p>for upper bound $ T(n) \u2264 c (n^2)$</p> <p>Basis for n = 1   $$ T(1) \u2264 C(1), Holds True$$</p> <p>So we can conclude that the eq 1 hold true for all value of n \u2265 1,   i.e 1, 2, 3, 4 ,.... Kth term</p> </li> <li> <p>step 3 : Inductive Hypothesis</p> <p>Assuming n  \u2265 k for all values 1 \u2264 k \u2264 n             $$ T(k) \u2264 c(k^2) $$</p> <p>Therefore $ K= n-1 $ should also hold true             $$ T(n-1) \u2264 c((n-1)^2) $$</p> <p>Thus, $$T(n) = T(n-1) + n $$           $$ \u2264 c((n-1)^{2}) + n $$           $$ \u2264 c(n^2 - 2n + 1)  + n $$           $$ \u2264 cn^2 - 2cn + c +n  $$           Hence proved</p> <p>OR futher checking  We want to show $ \ud835\udc47(\ud835\udc5b)\u2264\ud835\udc50\ud835\udc5b^2 $</p> <p>That means checking if:   $$ \ud835\udc50\ud835\udc5b^2\u22122\ud835\udc50\ud835\udc5b+\ud835\udc50+\ud835\udc5b  \u2264   \ud835\udc50\ud835\udc5b^2  $$</p> <p>Cancel $ \ud835\udc50\ud835\udc5b^2 $ on both sides:</p> <p>$$ \u22122\ud835\udc50\ud835\udc5b+\ud835\udc50+\ud835\udc5b  \u2264   0 $$  $$ n(1-2c)+c \u2264  0 $$</p> <p>If we choose $ \ud835\udc50 \u2265 1 $ then $ n(1-2c) \u2264 -1 $ so the inequality holds for all sufficiently large values</p> </li> </ul>"},{"location":"unit3.html#324-master-theorem","title":"3.2.4 Master Theorem","text":""},{"location":"unit3.html#1-master-theorem-classic","title":"1. Master Theorem (Classic)","text":"<p>Definition: The Master Theorem provides a method to determine the asymptotic time complexity of divide-and-conquer recurrences of the form:</p> \\[ T(n) = a \\, T\\left(\\frac{n}{b}\\right) + f(n) \\] <p>Where:</p> <ul> <li>\\(a \\ge 1\\) \u2192 number of subproblems</li> <li>\\(b &gt; 1\\) \u2192 factor by which the problem size is divided</li> <li>\\(f(n)\\) \u2192 non-recursive work done per call</li> </ul> <p> Cases</p> <p>Case 1: Recursion dominates: $$ if \\quad f(n) = O(n^{\\log_b a - \\epsilon}) \\; for \\; some \\; constant \\; \\epsilon &gt; 0  \\quad Result: T(n) = \\Theta(n^{\\log_b a}) $$</p> <p>Case 2: Balanced: $$ f(n) = \\Theta(n^{\\log_b a}) \\quad Result:T(n) = \\Theta(n^{\\log_b a} \\log n) $$</p> <p>case 3: Outside work dominates: $$ f(n) = \\Omega(n^{\\log_b a + \\epsilon}) \\; for \\; some \\; constant \\; \\epsilon &gt; 0  \\quad \\text{(with regularity condition)} \\quad Result:T(n) = \\Theta(f(n)) $$</p> <p>regualarity Condition: $$ af(\\frac{n}{b}) \\le cf(n) $$ for some constant c &lt; 1 and all sufficiently large n, then $$ Result : T(n) = \\Theta (f(n)) $$</p> <p>Steps for Master Theorem</p> <ul> <li>From the equation note the values of a, b and $ f(n)$</li> <li>calculate the thershold function $ n^{\\log_b a} $, let be denoted as $ g(n) $</li> <li>compare \\(f(n)\\) with \\(g(n)\\) and check which category it falls into</li> <li>Note down the final result</li> </ul> <p>Example 1</p> <p>Q1. $ T(n) = 4T(\\frac{n}{2}) + n $</p> <p>step 1: from the given equation noting the required value : $ a = 4, b = 2, f(n) = n$</p> <p>step 2: Calcuating Threshold function : $ g(n) = n^{\\log_b n} \\implies g(n) = n^{\\log_2 4} \\implies g(n) = n^2$</p> <p>step 3: compare the \\(f(n)\\) and $ g(n)$</p> <p>$$ f(n) \\le g(n) $$   $$ n \\le n^2 $$</p> <p>step 4: Case 1 condition satisfied $f(n) = O (n^{log_b a- \\epsilon}) $ where \\(\\epsilon &gt; 0\\) i.e, \\(\\epsilon = 1\\)</p> <p>step 5: Result : $ T(n) = \\Theta (n^ {log_b a} )$   $$   \\boxed{T(n) = \\Theta (n^ 2)}   $$</p> <p>Q2. $ T(n) = T(\\frac{n}{2}) + 1$</p> <p>step 1: from the given equation noting the required value : $ a = 1, b = 2, f(n) = 1$</p> <p>step 2: Calcuating Threshold function : $ g(n) = n^{\\log_b n} \\implies g(n) = n^{\\log_2 1} \\implies g(n) = n^0 = 1$</p> <p>step 3: compare the \\(f(n)\\) and $ g(n)$</p> <p>$$ f(n) = g(n) $$   $$ 1 = 1 $$</p> <p>step 4: Case 2 condition satisfied $f(n) = \\theta (n^{log_b a}) $</p> <p>step 5: Result : $ T(n) = \\Theta (n^ {log_b a} \\log n)$   $$   \\boxed{T(n) = \\Theta (log n)}   $$    Q3. $ T(n) = 2T(\\frac{n}{2}) + n^4$</p> <p>step 1: from the given equation noting the required value : $ a = 2, b = 2, f(n) = n^4$</p> <p>step 2: Calcuating Threshold function : $ g(n) = n^{\\log_b n} \\implies g(n) = n^{\\log_2 2} \\implies g(n) = n^1 = n$</p> <p>step 3: compare the \\(f(n)\\) and $ g(n)$</p> <p>$$ f(n) &gt; g(n) $$   $$ n^4 = n $$</p> <p>step 4: Case 3 : thus, go for regularity condition check   condition satisfied i.e, $af(\\frac{n}{b}) \\le cf(n) $</p> <p>$$af(\\frac{n}{b}) \\le cf(n) $$</p> <p>$$   2 \\cdot (n^{4} /2^{4}) \\le c \\cdot n^{4}   $$   $$    (\\frac{1}{8}) \\le c \\cdot 1   $$</p> <p>$\\therefore $ condition holds for the regularity check</p> <p>step 5: Result : $ T(n) = \\Theta (n^ {4} )$</p> <p>$$   \\boxed{T(n) = \\Theta (n^4)}   $$</p>"},{"location":"unit3.html#2-generalized-master-theorem","title":"2. Generalized Master Theorem","text":"<p>Definition: The Generalized Master Theorem generalizes the Master Theorem to handle recurrences where \\(f(n)\\) has logarithmic factors, i.e.,</p> \\[ T(n) = a \\, T\\left(\\frac{n}{b}\\right) + f(n) \\] <p>where</p> \\[ f(n) = \\Theta(n^{\\log_b a} \\cdot \\log^k n), \\quad k \\ge 0 \\] <p>Use this formula if there is no  or directly when you use log function in the addtional cost for other function <p>Intution behind the Generalized Master Theorem</p> <p>Lets say for example a recurrence relation is represented by \\(T(n) = 2T(\\frac{n}{2})+nlogn\\)</p> <p>if we used the Classic Master theorem than   $$ a= 2, b=2, f(n) = n \\log n$$</p> <ul> <li> <p>calculating the \\(g(n)\\), we get \\(g(n)= n\\)</p> </li> <li> <p>comparing \\(f(n)\\) and \\(g(n)\\), we can see that \\(f(n)\\) is greater than \\(g(n)\\) and we might say that CASE 3 applies and proceed to check the regularity condition. However, that would be an Error as the function \\(f(n)\\) is growing logrithmically with respect to \\(g(n)\\) and not polynomially. (see ). Thus classic master theorem cannot be applied to this recurrence relation. <li> <p>for such recurrence relations we use the generalized master theorem which takes into account the logarithmic factors</p> </li> <p>Cases</p> <p>Case 1: k &gt; -1 $$ T(n) = \\Theta(n^{\\log_b a} \\log^{k+1} n) $$</p> <p>Case 2: k = -1 $$ T(n) = \\Theta(n^{\\log_b a} \\log \\log n) $$</p> <p>Case 3: k &lt; 1 $$ T(n) = \\Theta(n^{\\log_b a} ) $$</p> <p>Example</p> <p>Q1. $ (T(n) = 2T(n/2) + n \\log n) $</p> <p>$$ a=2, b=2, f(n)= n \\log n k=1$$</p> <p>Classic Master theorem cannot be applied here thus, using the generalized Master theorem for \\(k=1\\)</p> <p>Result: \\(T(n) = \\Theta(n^{\\log_b a} \\log^{k+1} n)\\)</p> <p>Thus final Answer:</p> <p>$$   \\boxed{T(n) = \\Theta(n \\log^2 n)}   $$</p>"},{"location":"unit3.html#3-extended-master-theorem","title":"3. Extended Master Theorem","text":"<p>Definition:</p> \\[ T(n) = a \\, T\\left(\\frac{n}{b}\\right) + f(n) \\] <p>where</p> \\[ f(n) = \\Theta(n^k \\cdot \\log^p n), \\quad k \\ge 0 \\] <p>Steps for Master Theorem</p> <ul> <li>From the equation note the values of a, b , k and p</li> <li>calculate the $ b^k $</li> <li>compare \\(a\\) with \\(b^k\\) and check which category it falls into and the value of p</li> <li>Note down the final result</li> </ul> <p>Case 1: $ a &gt; b^k $ </p> \\[ T(n) = \\Theta(n^{\\log_b a}) \\] <p>Case 2: $ a = b^k $</p> <p>Then check the value of p</p> \\[ p &gt; -1 \\: then \\: result \\:  T(n) = \\Theta(n^{k} \\log^{p+1} n) \\] \\[ p = -1 \\: then \\: result \\:  T(n) = \\Theta(n^{k} \\log \\log n) \\] \\[ p &lt; -1 \\: then \\: result \\:  T(n) = \\Theta(n^{k} ) \\] <p>Case 3: $ a &lt; b^k $</p> <p>Then check the value of p</p> \\[ p \\ge 0 \\: then \\: result \\:  T(n) = \\Theta(n^{k} \\log^{p} n) \\] \\[ p &lt; 0 \\: then \\: result \\:  T(n) = \\Theta(n^{k}) \\]"},{"location":"unit3.html#4-change-of-variable-method","title":"4. Change of variable Method","text":"<p>There are cases where the given recurrence relation does not conform to the general form of recurrence relations, in such cases we can apply the change of variable method in order to solve it using the methods for solving recurrence relation.</p> <p>You can apply the change of variable method (also called variable substitution) in solving recurrence relations when:</p> <ul> <li> <p>Non-standard form \u2013 The recurrence doesn\u2019t fit directly into the Master Theorem or standard forms Example:$ T(n)=T(n\u22121)+n. $</p> </li> <li> <p>Divide-and-conquer recurrences with unusual terms \u2013 When the recurrence has logarithmic or polynomial distortions that prevent direct comparison.Example: $\ud835\udc47(\ud835\udc5b)=2\ud835\udc47(\\sqrt \ud835\udc5b)+\ud835\udc5b $ Here, substitution like $\ud835\udc5b = 2^\ud835\udc5a $ converts it into \ud835\udc47(2\ud835\udc5a) = 2\ud835\udc47(2\ud835\udc5a/2), which is easier to handle.</p> </li> <li> <p>When the variable appears in a non-linear way \u2013 Such as square roots, logarithms, or powers. Example: \ud835\udc47(\ud835\udc5b) =\ud835\udc47(\ud835\udc5b/2)+ log \ud835\udc5b. Substitution like \\(\ud835\udc5b  =  2^\ud835\udc5a\\), simplifies it to \\(\ud835\udc47(2^\ud835\udc5a) = \ud835\udc47(2^{\ud835\udc5a\u22121})+ \ud835\udc5a\\)</p> </li> <li> <p>When recurrence is expressed in terms of functions of \ud835\udc5b,like \ud835\udc47(log\u2061 \ud835\udc5b),\\(\ud835\udc47(\ud835\udc5b^2)\\), etc.</p> </li> </ul> <p>Example</p> <p>Say $T(n) = 2T(\\sqrt n) + log n $</p> <p>let $$ n = 2^m \\tag{1} $$</p> <p>now = $T(2^m) = 2T(\\sqrt {2^m}) + log 2^m $</p> \\[ \\implies T(2^m) = 2T(2^{\\frac{m}{2}}) + log 2^m \\] <p>let $ T(2^m) = S(m) $ then</p> \\[ \\implies S(\\frac{m}{2}) = T(2^{\\frac{m}{2}}) \\] \\[ \\implies S(m) = 2S(\\frac{m}{2}) +m \\tag{2}\\] <p>as Seen equation (2) is in a general form of recurrence relation, so we can solve this using master theorem  where \\(a=2, b=2\\) and \\(f(m) =m\\). on solving it falls in case 2, thus solution is \\(f(m) = \\Theta (m \\log m)\\)  substituting back in eq(1)</p> \\[n=2^m\\] \\[\\implies m= \\log n\\] <p>therefore $$   \\boxed{         f(n) =\\Theta(log log  logn)   } $$</p>"},{"location":"unit3.html#33-merge-sort-analysis","title":"3.3 Merge sort analysis","text":"<ul> <li>The sort was developed by the American computer scientist John von Neumann in 1945.</li> <li>It is a comparison-based sort.</li> <li> <p>It uses divide-&amp;-conquer approach. There are three steps in this approach.</p> <ul> <li>Divide the problem into a number of subproblems that are smaller instances of the         same problem.</li> <li>Conquer the subproblems by solving them recursively. If the subproblem sizes are         small enough, however, just solve the subproblems in a straightforward manner.</li> <li>Combine the solutions to the subproblems into the solution for the original problem.</li> </ul> </li> </ul> <p>Example :</p> <ol> <li>Given an array of unsorted numbers : Array = $ \\begin{bmatrix} 2 &amp; 6 &amp; 8 &amp; 2 &amp; 3 &amp; 9 &amp; 1 &amp; 4 &amp; 9 \\end{bmatrix}$</li> <li>From the given array, divide the array in half i.e., This initial step separates the overall list into two smalle halves.</li> <li>Then, the lists are broken down further until they can no longer be divided, leaving only one element item in each halved list as in image 1.</li> </ol> <p></p> <ol> <li>In the conquer step, try to sort both the subarrays</li> <li>When the conquer step reaches the base step and we get two sorted subarrays, we combine the results by creating a sorted array from two sorted subarrays as in image 2.</li> </ol> <p></p> <p>Image : Image Source</p> <p>Pseudo Code for Merge Sort</p> <pre><code>MergeSort(Array[],low, high)\nif low &lt; high\nint mid &lt;- (low + high)/2\nMergeSort(Array[], low, mid)\nMergeSort(Array[], mid+1, high)\nMerge(Array[],low, mid, mid+1, high)\n\nMerge Function\nMerge(Array[],low, mid, mid+1, high)\nk =low; i =low; j =mid+ 1;\nwhile ((k &lt; mid) and (j &lt; high)) do\n  {\n        if (a[k] &lt; a[j])then\n              b[i] :=a[k];k:=k + 1;\n        else\n              b[i]-=a[j]; j :=j+ l;\n        i=i + 1;\n  }\n\n  if (i &gt; mid) then\n  for k :=j to high do\n  {\n  b[i]:=a[k];i :=i + 1;\n  else}\n  for k :=k to middo\n  {\n  b[i] :=a[k];i :=i + 1;\n  }\n  for k :=low to high doa[k] :=b[k];\n</code></pre> <p>Unit 3 Summary</p> <ol> <li>A recurrence is an equation (or inequality) that defines a function in terms of its value(s) at smaller input(s)</li> <li>A recurrence relation is an equation that defines a sequence based on its previous terms</li> <li>General form of Recurrence Relation:         $$         T(n) = a \\cdot T\\left(\\frac{n}{b}\\right) + f(n)         $$</li> </ol> <p>Where:</p> <ul> <li> <p>\\(T(n)\\) \u2192 the time/space complexity of the problem of size n</p> </li> <li> <p>\\(a\\) \u2192 number of subproblems in the recursion</p> </li> <li> <p>\\((n/b)\\) \u2192 size of each subproblem</p> </li> <li> <p>\\(f(n)\\) \u2192 the cost of dividing the problem and combining the results</p> </li> <li> <p>Method of solving Reucurrence relations : Iterative Method, Recursion tree, Substitution Method, Telescoping, Master theorem</p> </li> <li>Iterative Method : expand step by step and replace the recurrence with successive substitutions until a clear pattern emerges, stop once the base case is reached, then simplify.</li> <li>recurrsion Tree Method : expand step by step and replace the recurrence with successive substitutions until a clear pattern emerges, stop once the base case is reached, then simplify.</li> </ul>"},{"location":"unit3.html#references","title":"References","text":""},{"location":"unit4.html","title":"Macro Syntax Error","text":"<p>File: <code>unit4.md</code></p> <p>Line 102 in Markdown file: unexpected '}' <pre><code>            int A[n][n] = {{1,2,3}, {4,5,6},{7,8,9}};\n</code></pre></p>"},{"location":"unit5.html","title":"Unit V: Sorting Algorithms and Analysis","text":""},{"location":"unit5.html#51-comparison-based-sorting-analysis","title":"5.1 Comparison-based Sorting Analysis","text":"<p>These algorithms determine the sorted order by comparing elements. The fundamental operation is the \"compare-and-swap.\" Examples: Insertion Sort, Selection Sort, Bubble Sort, Merge Sort, Heap Sort, Quick Sort.</p>"},{"location":"unit5.html#511-common-comparison-based-sorting-algorithms","title":"5.1.1 Common Comparison based sorting Algorithms","text":"<p>Simple Sorts:</p> <ol> <li> <p>Insertion Sort: Builds the final sorted array one item at a time. It is efficient for small datasets or nearly sorted data.</p> </li> <li> <p>Selection Sort: Repeatedly finds the minimum element from the unsorted part and puts it at the beginning i.e, minimizes number of swaps</p> </li> <li> <p>Bubble Sort: Simplest Comparison based sorting algorithm, Repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.</p> </li> </ol> Algorithm Best Case Average Case Worst Case Stable In-place Insertion Sort O(n) O(n\u00b2) O(n\u00b2) Yes Yes Selection Sort O(n\u00b2) O(n\u00b2) O(n\u00b2) No Yes Bubble Sort O(n) O(n\u00b2) O(n\u00b2) Yes Yes <p>Note</p> <p>Insertion Sort (Best Case O(n)): Occurs when the input is already sorted. Each new element is only compared to its immediate predecessor and immediately placed, leading to n-1 comparisons and 0 swaps.</p> <p>Selection Sort (Always \u0398(n\u00b2)): It always performs ~n\u00b2/2 comparisons, regardless of input order, because it must scan the entire unsorted segment to find the minimum each time.</p> <p>Efficient Sorts:</p> <ol> <li> <p>Merge Sort: A divide-and-conquer algorithm that splits the array in half, recursively sorts each half, and then merges the two sorted halves.</p> </li> <li> <p>Heap Sort: Builds a binary max-heap from the input and then repeatedly extracts the maximum element, which gives the items in sorted order.</p> </li> </ol> Algorithm Best Case Average Case Worst Case Stable In-place Merge Sort O(n log n) O(n log n) O(n log n) Yes No Heap Sort O(n log n) O(n log n) O(n log n) No Yes <p>Note</p> <p>Merge Sort: The recurrence relation is \\(T(n) = 2T(n/2) + \u0398(n)\\). Applying the Master Theorem (Case 2), this solves to \u0398(n log n). The linear \u0398(n) term comes from the merge operation. The space complexity O(n) is for the temporary array used during merging.</p> <p>Heap Sort: Building the heap takes O(n) time. Each of the n extract-max operations takes O(log n) time. Thus, the total is O(n) + O(n log n) = O(n log n).</p>"},{"location":"unit5.html#52-lower-bound-of-sorting","title":"5.2 Lower Bound of Sorting","text":"<p>A lower bound is the best possible (minimum) time complexity that any algorithm of a certain type can achieve for a given problem.</p> <p>All comparison-based sorting algorithms (like Merge Sort, Quick Sort, Heap Sort, etc.) decide the order of elements by comparing pairs of elements (using &lt;, &gt;). Every sorting process can be seen as a series of decisions \u2014 each comparison narrows down the possible orderings of the input.</p>"},{"location":"unit5.html#521-decision-tree-model-for-comparison-based-sorting","title":"5.2.1 Decision tree model for comparison-based sorting","text":"Example of a decision Tree <p>Image Source:  NUS Notes</p> <p>A decision tree is a rooted tree</p> <ul> <li>Start from the root.</li> <li>At every node, a question is asked.</li> <li>Depending on the answer, a child is chosen.</li> <li>At a leaf, a decision is taken.</li> </ul> <p>Similarly any comparison-based algorithm can be modeled using a decision tree:</p> <ul> <li>Internal Nodes: Represent a comparison between two elements (e.g., a\u1d62 \u2264 a\u2c7c?).</li> <li>Branches: Represent the outcome of the comparison (Yes/No).</li> <li>Leaves: Represent one of the possible n! permutations of the input. A correct sorting algorithm must have a unique leaf for each permutation.</li> </ul> Example of a decision Tree <p>Image Source:  NUS Notes</p> <p>Proof: The worst-case time complexity of any comparison-based sorting algorithm is \u03a9 (\ud835\udc5b log \ud835\udc5b) .</p> <p>Sterling Approximation</p> <p>Stirling's approximation formula for the factorial of a large integer \\(n\\) is \\(n!\\approx \\sqrt{2\\pi n}(\\frac{n}{e})^{n}\\). This approximation is used to estimate large factorials that would be difficult or impossible to calculate directly, providing a close approximation to the actual value.</p> <ol> <li> <p>The decision tree must have at least n! leaves to account for all possible input permutations.</p> </li> <li> <p>A binary tree of height h has at most 2\u02b0 leaves. Therefore, 2\u02b0 \u2265 n!. (Remember: height of the decision tree (number of comparisons in the worst case) )</p> </li> <li> <p>Taking logarithms: h \u2265 log\u2082(n!).</p> <p>The factorial function \\(\ud835\udc5b!\\) grows extremely fast. It\u2019s hard to directly compute or simplify \\(log_2 (\ud835\udc5b!)\\) in terms of \ud835\udc5b</p> </li> <li> <p>Using Stirling's Approximation $$ n! \u2248 \\sqrt {2\u03c0n}(\\frac{n}{e})\u207f $$,</p> <p>we get:</p> </li> </ol> \\[     \\begin{align*}     \\log_2(n!) &amp;\\approx \\log_2(\\sqrt{2\\pi n}) + n\\log_2\\!\\left(\\frac{n}{e}\\right) \\\\      &amp;\\approx n\\log_2 n - n\\log_2 e + \\tfrac{1}{2}\\log_2(2\\pi n) \\\\      &amp;\\approx n\\log_2 n - n\\log_2 e + O(\\log n)      \\end{align*} \\] <p>The other terms are much smaller in magnitude compared to $\ud835\udc5b log_2 \ud835\udc5b $</p> <p>This simplifies to \\(h = \u03a9(n log n)\\).</p> <p>Conclusion: The height of the decision tree (which represents the number of comparisons in the worst case) is \u03a9(n log n).</p>"},{"location":"unit5.html#522-implications-and-limitation-of-lower-bound","title":"5.2.2 Implications and limitation of lower bound","text":"<p>Merge Sort and Heap Sort are asymptotically optimal comparison-based sorts because their worst-case running time O(n log n) matches the lower bound \u03a9(n log n). It is impossible to create a general-purpose comparison sort that is faster than this in the worst case.</p> <p>Limitations</p> <p>This proof only applies to comparison-based sorts. It does not apply to algorithms that leverage other information about the data, such as:</p> <ul> <li>The integer structure of keys (e.g., Counting Sort, Radix Sort).</li> <li>The distribution of the keys.   These \"non-comparison\" sorts can achieve O(n) time, breaking the n log n barrier.</li> </ul>"},{"location":"unit5.html#53-quick-sort-analysis-and-and-peformance","title":"5.3 Quick Sort Analysis and and Peformance","text":"<p>Quicksort is a divide-and-conquer algorithm.</p> <p>Divide: Choose a pivot element and partition the array so that all elements less than the pivot are to its left, and all elements greater are to its right. The pivot is now in its final sorted position.</p> <p>Conquer: Recursively sort the left and right sub-arrays.</p> <p>Combine: No work is needed, as the array is sorted in place.</p> Quick sort Algorithm <p>Image Source:  Strucrex</p> <p>Steps</p> <p>Step 1: Choose a Pivot</p> <p>Select an element from the array as the pivot. Common strategies:</p> <ul> <li>First element</li> <li>Last element</li> <li>Middle element</li> <li>Random element</li> <li>Median of three</li> </ul> <p>Step 2: Partitioning</p> <p>Rearrange the array so that:</p> <ul> <li>All elements less than pivot come before it</li> <li>All elements greater than pivot come after it</li> <li>The pivot is in its final sorted position</li> </ul> <p>Step 3: Recursively Sort</p> <p>Apply the same process to the sub-arrays on left and right of the pivot.</p>"},{"location":"unit5.html#54-randomized-pivot-selection-in-quicksort","title":"5.4 Randomized Pivot Selection in Quicksort","text":""},{"location":"unit6.html","title":"UNIT 6: DYANMIC PROGRAMING","text":"<p>Learning Outcome</p> <p>Design dynamic programming solutions for complex problems such as the rod cutting problem and optimal binary search trees.</p>"},{"location":"unit6.html#60-introduction","title":"6.0 Introduction","text":"<p>Dynamic programming is an algorithm design technique with a rather inter- esting history. It was invented by a prominent U.S mathematician, Richard Bellman, in the 1950s as a general method for optimizing multistage decision processes<sup>1</sup>.</p> <p>Dynamic Programming (DP) is a problem-solving paradigm used for optimization problems where the solution can be built from solutions to overlapping subproblems. It combines recursion and memory optimization.</p> <p>Overlapping Subproblems</p> <p>A problem exhibits overlapping subproblems if the same subproblems recur multiple times. Instead of recomputing, DP stores the results and reuses them.</p> <p>Example:</p> Fibonacci Sequence Tree <p>Image Source:  Stack Exchnage</p> <p>Fibonacci sequence (recursive) repeatedly solves the same subproblems (e.g., Fib(3) is computed multiple times). Using DP, we compute each Fibonacci number once and store it.</p> <p>Memoization vs Tabulation Approaches</p> <p>Memoization (Top-Down): Recursive calling + caching results i.e., Compute on demand and store results.Easier to implement if recursion is natural.</p> <p>Tabulation Approch(Bottom-up approch): Stores the results of subproblems in a table Iterative implementation. Entries are filled in a bottom-up manner from the smallest size to the final size.</p>"},{"location":"unit6.html#61-rod-cutting-algorithm","title":"6.1 Rod Cutting Algorithm","text":"<p>Given a rod of length \\(n\\) inches and a table of prices \\(P_i\\) for \\(i = 1, 2, . . ., n\\), determine the maximum revenue \\(r_n\\) obtainable by cutting up the rod and selling the pieces. Note that if the price \\(P_n\\) for a rod of length \\(n\\) is large enough, an optimal solution may require no cutting at all.<sup>2</sup></p> <p>Example</p> <p>Consider \\(n = 4\\) and pricing for each length as follows</p> <p> </p> <p>The Following figure shows all the ways how \\(n=4\\) can be cut  </p> <p>We can cut up a rod of length n in \\(2^{n-1}\\) different ways, since we have an independent option of cutting, or not cutting, at distance i inches from the left end for i = 1, 2, . . . , n. We denote a decomposition into pieces using ordinary additive notation, so that 7 = 2 + 2 + 3 indicates that a rod of length 7 is cut into three pieces\u2014two of length 2 and one of length 3.</p> <p>If an optimal solution cuts the rod into k pieces, for some \\(1 \\le k \\le n\\), then an optimal decomposition \\(n = i_1 + i_2 + . . . + i_k\\) of the rod into pieces of lengths \\(i_1, i_2, . . . , i_k\\) provides maximum corresponding revenue \\(r_n = p_{i_1} + p_{i_2} + . . . +  p_{i_k}\\).</p> <p>Recursive formulation:</p> \\[ R(n) = \\max_{1 \\leq i \\leq n} \\{\\, P_i + R_{n-i} \\}, \\quad R(0) = 0 \\] <p>Recursive top-down implementation</p> Pseudocode for Rod cutting Algorithm using recursive top-down approach <p>Image Source: Introduction to Algorithms, Chapter 15</p> <ol> <li>CUT-ROD takes as input an array p[1 .. n] of prices and an integer n and returns the maximum revenue possible for a rod of length n</li> <li>If n = 0, no revenue is possible</li> <li>Line 3 initializes the maximum revenue q to -\u221e, so that the for loop in lines 4\u20135 correctly computes $R(n) = max_{1 \\leq i \\leq n} ( P_i + CUT-ROD (p,{n-i})) $</li> <li> <p>line 6 then returns this value</p> </li> <li> <p>Exponential time complexity \\(O(2^n)\\).</p> </li> <li>Many repeated computations \\(R_{n-1}\\) etc. appear multiple times.</li> </ol> <p>Using dynamic programming for optimal rod cutting</p> <p>we arrange for each subproblem to be solved only once, saving its solution. If we need to refer to this subproblem\u2019s solution again later, we can just look it up, rather than recompute it. Dynamic programming thus uses additional memory to save computation time.</p> <ol> <li> <p>The first approach is top-down with memoization</p> <ul> <li>we write the procedure recursively in a natural manner, but modified to save the result of each subproblem (usually in an array or hash table).</li> <li>The procedure now first checks to see whether it has previously solved this subproblem.</li> <li>If yes, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner.</li> </ul> <p>For the Rod-cutting Problem</p> <pre><code>MEMOIZED-CUT-ROD(p, n)\n1 let r[0.. n] be a new array\n2 for i = 0 to n\n3   r[i] = -\u221e\n4 return MEMOIZED-CUT-ROD-AUX(p,n,r)\n\nMEMOIZED-CUT-ROD-AUX(p,n,r)\n1 if r[n] \u2265 0\n2   return r[n]\n3 if n ==\n4 q = 0\n5 else q = -\u221e\n6   for i = 1 to n\n7       q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n-i, r))\n8 r[n] = q\n9 return\n</code></pre> </li> <li> <p>The second approach is the bottom-up method.</p> <ul> <li>This approach typically depends on some natural notion of the \u201csize\u201d of a subproblem, such that solving any particular subproblem depends only on solving \u201csmaller\u201d subproblems.</li> <li>We sort the subproblems by size and solve them in size order, smallest first.</li> <li>When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.</li> <li>We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems</li> </ul> </li> </ol> Pseudocode for Rod cutting Algorithm Buttom Up approach <p>Image Source: Introduction to Algorithms, Chapter 15</p> <p>example</p> <p>Given length of Rod= 5 m</p> Length 1 2 3 4 P[i] 2 5 7 8 <p>Solution using Buttom up method</p> <ul> <li>If a rod has a length of 0, what would the maximum profit be? It would be 0\u200b.</li> </ul> cost Piece 0 1 2 3 4 5 \u2190 Possible Length of pieces - - - \"\" \"\" \"\" \"\" \"\" 2 1 0 \"\" \"\" \"\" \"\" \"\" 5 2 0 \"\" \"\" \"\" \"\" \"\" 7 3 0 \"\" \"\" \"\" \"\" \"\" 8 4 0 \"\" \"\" \"\" \"\" \"\" <ul> <li> <p>In the first row, we only have the price 2 for a rod of length 1 in our array.</p> <p>we will use i = row index  and  j= column index and the cell \\([i,j]\\) represents the maximum amount of profit that can be earned by selling a rod of length j. Example: \\([2,3]\\) represents the masximum profit that can be earned with a rod of length 3, with differnt length sub rods {0,1,2}</p> <p>At index [0,1], making a cut at length 1 yields a profit of 2.  Not making any cut also gives a profit of 2, since only a 1-meter rod exists.</p> <p>At index [0,2], making a cut at length 1 results in two pieces of length 1 each, giving a total profit of 4 (2 + 2).  Without any cut, the profit would also be 4\u2014not 5, because the price for a 2-meter rod hasn\u2019t been introduced yet in the array.</p> <p>Therefore, the better option (and the maximum profit) at this stage is 4.</p> <p>Repeating it for the entire row will result as follows:</p> </li> </ul> cost Piece 0 1 2 3 4 5 - - - 0 0 0 0 0 2 1 0 [0,0] 2 [0,1] 4 [0,2] 6 [0,3] 8 [0,4] 10 [0,5] <ul> <li> <p>At each step, we decide whether adding a new cut mark (at position i + 1) increases the profit for the given rod length j. We make this decision by taking the maximum of two possible cases:</p> <ul> <li> <p>Without the new cut mark: The maximum profit before the new cut mark was added \u2014 represented by the value at\\([i \u2212 1, j]\\). This corresponds to ignoring the newly introduced cut mark.</p> </li> <li> <p>With the new cut mark: The price of the current cut \\((price[i])\\) plus the maximum profit obtainable from the remaining rod of length \\(j \u2212 i \u2212 1\\), which is represented by the value at \\([i, j \u2212 i \u2212 1]\\) (and would already have been computed earlier).This represents the case where the new cut mark contributes to the profit.</p> </li> </ul> </li> </ul> <p>In essence, we are continuously comparing whether including the latest cut leads to a higher profit than excluding it.</p> <p>so the general expression used for filling the cell is :</p> \\[P[i,j] = max \\begin{cases} P[i-1,j]  \\\\ P[i] + P[i, j-i-1] \\end{cases} \\] <p>in short : Profit = max{Profit excluding the new Piece, profit including the new piece}</p> <ul> <li>Thus, with this condition, if we fill the table it would be as follows:</li> </ul> cost Piece 0 1 2 3 4 5 - - - 0 0 0 0 0 2 1 0 2 4 6 8 10 5 2 0 2 5 7 10 12 7 3 0 2 5 7 10 12 8 4 0 2 5 7 10 12 <ul> <li>The last cell at the bottom right corner gives the maximum profit that can be earned for the given length of rod</li> </ul>"},{"location":"unit6.html#62-matrix-chain-multiplication","title":"6.2 Matrix Chain Multiplication","text":"<p>Given a chain \\({A_1, A_2, A_3, .. A_n}\\) of \\(n\\) matrices, where for \\(i = 1,2,3 ... n\\), matrix \\(A_i\\) has dimension \\(p_{i-1}, p_i\\), fully parenthesize the product \\({A_1, A_2, A_3, .. A_n}\\) in a way that minimizes the number of scalar multiplications i.e, given a sequence of Matrix we want to find the best sequence to multiply them together.</p> <p>Remember</p> <p>Matrix multiplication as associative in nature but not commutative.</p> <p>Example: \\(A \\cdot (B \\cdot C)\\) = \\((A \\cdot B ) \\cdot C\\) but \\(A \\cdot B \\not = B \\cdot A\\)</p> <p>Although the matrix multiplication is associative in nature, however the number of computation (scalar Multiplication) depends on he sequence of operation that is done.</p> <p>exmple</p> <p>Tips</p> <ol> <li>Multiplying two matrix \\(A_{a \\times b}\\) and \\(B_{b \\times c}\\), the product will have a dimention of \\(R_{a \\times c}\\)</li> <li>Multiplying two matrix \\(A_{a \\times b}\\) and \\(B_{b \\times c}\\), number of multiplication = \\(a \\times b \\times c\\)</li> </ol> <p>Lets say there are three Matrix \\(A_{10 \\times 5},\\quad B_{5\\times 20},\\quad C_{20\\times 10}\\), so we have two ways to multiply it, therefore the required number of computation is :</p> <ol> <li> <p>\\((A \\cdot B ) \\cdot C\\)</p> <p>=\\((10 \\times 5 \\times 20)\\) + \\((10 \\times 20 \\times 10)\\)</p> <p>= 1000 + 2000</p> <p>= 3000</p> </li> <li> <p>\\(A \\cdot (B  \\cdot C)\\)</p> <p>=\\((10 \\times 5 \\times 10)\\) + \\((5 \\times 20 \\times 10)\\)</p> <p>= 500 + 1000</p> <p>= 1500</p> </li> </ol> <p>Although both the matrix multiplications will yield the same result, the number of multiplication required will depend on the sequence or optimal parenthesization of the sequence of array.</p> <p>The Recursive definition for the minimum cost of parenthesizing the product \\(A_i,\\quad A_{i+1} \\quad ... A_j\\) becomes</p> \\[ m[i, j] = \\begin{cases}     0, &amp; \\text{if } i = j \\\\     \\displaystyle \\min_{i \\le k &lt; j} \\{\\, m[i,k] + m[k+1,j] + P_{i-1} \\cdot P_k \\cdot P_j \\,\\}, &amp; \\text{if } i &lt; j \\end{cases} \\] <p>Example</p> \\[ \\begin{array}{c|cccc} \\text{Matrix} &amp; A_1 &amp; A_2 &amp; A_3 &amp; A_4 \\\\ \\hline \\text{Dimension} &amp; 5 \\times 4 &amp; 4 \\times 6 &amp; 6 \\times 2 &amp; 2 \\times 7 \\end{array} \\] <p>Here, \\(P = [5, 4, 6, 2, 7]\\) represents the matrix dimensions for the chain \\(A_1 A_2 A_3 A_4\\), where matrix \\(A_i\\) has dimension \\(P_{i-1} \\times P_i\\).</p> <p>1) Minimum Multiplication Cost Table \\(m[i,j]\\)</p> <ul> <li>\\(m[1,2] = 5 \\cdot 4 \\cdot 6 = 120\\)</li> <li>\\(m[2,3] = 4 \\cdot 6 \\cdot 2 = 48\\)</li> <li>\\(m[3,4] = 6 \\cdot 2 \\cdot 7 = 84\\)</li> </ul> i  j 1 2 3 4 1 0 120 - - 2 0 48 - 3 0 84 4 0 <p>Split Table \\(s[i,j]\\) (Optimal split positions)</p> i  j 1 2 3 4 1 - 1 - - 2 - 2 - 3 - 3 4 - <p>2) Minimum Multiplication Cost Table \\(m[i,j]\\)</p> <p>\\(m[1,3]\\):</p> <ul> <li>\\(k=1: 0 + 48 + 5\\cdot4\\cdot2 = 88\\) \u2190 optimal Solution</li> <li>\\(k=2: 120 + 0 + 5\\cdot6\\cdot2 = 180\\)</li> </ul> <p>\\(m[2,4]\\):</p> <ul> <li>\\(k=2: 0 + 84 + 4\\cdot6\\cdot7 = 252\\)</li> <li>\\(k=3: 48 + 0 + 4\\cdot2\\cdot7 = 104\\) \u2190 optimal Solution</li> </ul> i  j 1 2 3 4 1 0 120 88 - 2 0 48 104 3 0 84 4 0 <p>Split Table \\(s[i,j]\\) (Optimal split positions)</p> i  j 1 2 3 4 1 - 1 1 - 2 - 2 3 3 - 3 4 - <p>3) Minimum Multiplication Cost Table \\(m[i,j]\\)</p> <p>Full chain \\(m[1,4]\\):</p> <ul> <li>\\(k=1: 0 + 104 + 5\\cdot4\\cdot7 = 244\\)</li> <li>\\(k=2: 120 + 84 + 5\\cdot6\\cdot7 = 414\\)</li> <li>\\(k=3: 88 + 0 + 5\\cdot2\\cdot7 = 158\\) \u2190 optimal Solution</li> </ul> <p>\\(m[1,4]=158\\) is the minimum number of scalar multiplications to compute \\(A_1A_2A_3A_4\\).</p> i  j 1 2 3 4 1 0 120 88 158 2 0 48 104 3 0 84 4 0 <p>Interpretation</p> <p>e.g., \\(s[1,3]=1\\) means the optimal split for \\(m[1,3]\\) is at \\(k=1\\) (i.e., \\((A_1)(A_2A_3)\\)).</p> i  j 1 2 3 4 1 - 1 1 3 2 - 2 3 3 - 3 4 - <p>Therefore, optimal Parenthesization : \\(( (A_1 \\times (A_2 \\times A_3)) \\times A_4 )\\)</p>"},{"location":"unit6.html#reference","title":"Reference","text":"<ol> <li> <p>Introduction to the Design and Analysis of Algorithms by Anany Levitin\u00a0\u21a9</p> </li> <li> <p>Cormen, T., Leiserson, C., Rivest, R., &amp; Stein, C. (2009). Introduction to Algorithms (Third). Mit Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"template/topic.html","title":"1.1 Topic","text":""},{"location":"template/topic.html#111-sub-topic-1","title":"1.1.1 sub topic 1","text":"<p>Brief explanation of the concept and its context within the unit.</p>"},{"location":"template/topic.html#111-sub-topic-2","title":"1.1.1 sub topic 2","text":"<p>Brief explanation of the concept and its context within the unit.</p>"},{"location":"template/topic.html#1111-sub-topic-of-sub-topic","title":"1.1.1.1 SUb topic of sub topic","text":"<ul> <li>Definition: ...</li> <li>Formula / Rule:   $$ E = mc^2 $$</li> <li>Example: <pre><code># Sample code illustrating the concept\n</code></pre></li> </ul>"},{"location":"template/topic.html#reference","title":"reference","text":""},{"location":"template/topic.html#test-your-self","title":"Test Your self","text":""},{"location":"unit6/overview.html","title":"Overview","text":""},{"location":"unit6/overview.html#learning-outcomes","title":"Learning Outcomes","text":"<p>Learning Outcome</p> <p>\ud83c\udfaf Define key concepts in number theory, combinatorics, and probability theory as they relate to algorithmic design.</p>"},{"location":"unit6/topic1.html","title":"1.1 Number Theory","text":""},{"location":"unit6/topic1.html#111-prime-numbers-optimized-trial-division-modified-sieve","title":"1.1.1 Prime Numbers, Optimized Trial Division, Modified Sieve","text":""},{"location":"unit6/topic1.html#112-greatest-common-divisor-least-common-divisor","title":"1.1.2 Greatest Common Divisor &amp; Least Common Divisor","text":""},{"location":"unit6/topic1.html#113-modular-arithmetic-extended-euclidean-algorithm","title":"1.1.3 Modular Arithmetic, Extended Euclidean Algorithm","text":""},{"location":"unit6/topic1.html#114-number-theory-in-programming-competitions","title":"1.1.4 Number Theory in Programming Competitions","text":""},{"location":"unit6/topic1.html#reference","title":"Reference","text":""},{"location":"unit6/topic1.html#test-your-self","title":"Test Your self","text":""},{"location":"unit6/unit1.html","title":"Unit 1: Basic Mathematical Foundation","text":"<p>Learning Outcome</p> <p>Define key concepts in number theory, combinatorics, and probability theory as they relate to algorithmic design.</p> <p>The following note are some of the fundamental mathematical formulas that will be used in calculations (optional Reading if you are already fimiliar with it)</p>"},{"location":"unit6/unit1.html#11-logarithms","title":"1.1 Logarithms","text":"<ol> <li> <p>Types of log :</p> <ul> <li>Binary Log : $ \\log_2 n$</li> <li>Natural Log : $ \\ln n = \\log_e n$</li> <li>Exponentiation : $ \\log^k n = (\\log n)^k$</li> <li>Composition: $ \\log \\log n = \\log(\\log n)$</li> </ul> </li> </ol>  2.  $$ a = b^{\\log_b a}$$  3.  $$\\log_c (ab) = \\log_c a + \\log_c b  $$ 4.  $$\\log_b a^n = n \\cdotp \\log_b a$$ 5.  $$\\log_b a = \\frac{\\log_c a}{\\log_c b}   $$ 6.  $$\\log_b \\frac{1}{a} = \u2212 \\log_b a$$ 7.  $$\\log_b a =\\frac{1}{log_a b} $$ 8.  $$ a^{\\log_b c} = c^{\\log_b a} $$"},{"location":"unit6/unit1.html#12-sequence-and-series","title":"1.2 Sequence and Series","text":"<ol> <li>Sum of Arithmatic Series</li> </ol> \\[ 1 + 2 + \\dots + n = \\frac{n (n + 1)}{2} \\] \\[ 1 + 2^2 + \\dots + n^2 = \\frac{n (n + 1)(2n + 1)}{6} \\] \\[ 1 + a + a^2 + \\dots + a^n = \\frac{a^{\\,n+1} - 1}{a - 1} \\] \\[ a + ar + ar^2 + ar^3 + \\dots + ar^n = \\frac{a(r^{\\,n+1} - 1)}{r - 1} \\]"},{"location":"unit6/unit1.html#1-exponentials","title":"1. Exponentials","text":"1. $$ a^{-1} = \\frac{1}{a} $$  2. $$(a^{m})^{n} = a^{m \\cdot n}$$  3. $$a^{m} \\cdot a^{n} = a^{m + n}$$"}]}